import re
import time  # ç¡®ä¿å¯¼å…¥timeæ¨¡å—
import hashlib  # ç”¨äºç”Ÿæˆç¼“å­˜é”®

import numpy as np
import faiss
from abc import ABC, abstractmethod
from typing import List, Optional, Dict
from openai import OpenAI
from sentence_transformers import SentenceTransformer, CrossEncoder

"""
æœ¬æ–‡ä»¶ä¾èµ–å®‰è£…
pip install sentence-transformers faiss-cpu numpy openai
å¦‚æœä½¿ç”¨åœ¨çº¿æ¨¡å‹éœ€è¦é¢å¤–å®‰è£…openaiç­‰å¯¹åº”SDK
"""


class EmbeddingModel(ABC):
    @abstractmethod
    def embed(self, texts: List[str]) -> List[List[float]]:
        pass


class LocalEmbeddingModel(EmbeddingModel):
    def __init__(self, model_path: str):
        self.model = SentenceTransformer(model_path)
        self.cache = {}  # æ·»åŠ ç¼“å­˜å­—å…¸

    def embed(self, texts: List[str]) -> List[List[float]]:
        results = []
        uncached_texts = []
        uncached_indices = []
        
        # æ£€æŸ¥ç¼“å­˜
        for i, text in enumerate(texts):
            # ä½¿ç”¨MD5ç”Ÿæˆç¼“å­˜é”®
            cache_key = hashlib.md5(text.encode('utf-8')).hexdigest()
            if cache_key in self.cache:
                results.append(self.cache[cache_key])
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # å¦‚æœæœ‰æœªç¼“å­˜çš„æ–‡æœ¬ï¼Œåˆ™åµŒå…¥
        if uncached_texts:
            embeddings = self.model.encode(uncached_texts, convert_to_tensor=False).tolist()
            
            # æ›´æ–°ç¼“å­˜å¹¶å¡«å……ç»“æœ
            for i, text in enumerate(uncached_texts):
                cache_key = hashlib.md5(text.encode('utf-8')).hexdigest()
                self.cache[cache_key] = embeddings[i]
                results.insert(uncached_indices[i], embeddings[i])
                
        return results


class OnlineEmbeddingModel(EmbeddingModel):
    def __init__(self, model_name: str, api_key: Optional[str] = None, base_url: Optional[str] = None):
        self.model_name = model_name
        self.api_key = api_key
        self.base_url = base_url
        self.cache = {}  # æ·»åŠ åµŒå…¥ç¼“å­˜
        self.cache_hits = 0  # ç¼“å­˜å‘½ä¸­è®¡æ•°
        self.api_calls = 0  # APIè°ƒç”¨è®¡æ•°
        
        # å¢åŠ é…ç½®æ—¥å¿—
        print(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {model_name}")
        print(f"API URL: {base_url if base_url else 'é»˜è®¤OpenAIåœ°å€'}")
        print(f"APIå¯†é’¥: {api_key[:6]}...{api_key[-4:] if api_key and len(api_key) > 10 else 'æœªè®¾ç½®'}")
        
        try:
            self.client = OpenAI(
                base_url=self.base_url,
                api_key=self.api_key
            )
            # æµ‹è¯•è¿æ¥
            self.client.models.list()
            print("âœ… APIè¿æ¥æµ‹è¯•æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ APIåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            # ä»ç„¶åˆ›å»ºå®¢æˆ·ç«¯ï¼Œä½†æ ‡è®°çŠ¶æ€
            self.client = OpenAI(
                base_url=self.base_url,
                api_key=self.api_key
            )

    def embed(self, texts: List[str]) -> List[List[float]]:
        if not texts:
            return []

        embeddings = []
        for text in texts:
            if not text.strip():
                embeddings.append([])
                continue
                
            # ä½¿ç”¨æ–‡æœ¬çš„MD5å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
            cache_key = hashlib.md5(text.encode('utf-8')).hexdigest()
            
            # æ£€æŸ¥ç¼“å­˜
            if cache_key in self.cache:
                self.cache_hits += 1
                cached_percent = (self.cache_hits / (self.cache_hits + self.api_calls)) * 100 if (self.cache_hits + self.api_calls) > 0 else 0
                print(f"ğŸ“‹ ç¼“å­˜å‘½ä¸­: {text[:20]}... (å‘½ä¸­ç‡: {cached_percent:.1f}%)")
                embeddings.append(self.cache[cache_key])
                continue
                
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œéœ€è¦è°ƒç”¨API
            for attempt in range(3):  # æœ€å¤šé‡è¯•3æ¬¡
                try:
                    # å¢åŠ è¯·æ±‚è°ƒè¯•ä¿¡æ¯
                    print(f"å‘é€åµŒå…¥è¯·æ±‚ (å°è¯• {attempt+1}/3):")
                    print(f"  - æ¨¡å‹: {self.model_name}")
                    print(f"  - æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
                    
                    response = self.client.embeddings.create(
                        model=self.model_name,
                        input=text,
                        encoding_format="float"
                    )
                    self.api_calls += 1

                    # å¼ºåŒ–å“åº”æ ¡éªŒ
                    if not response or not response.data:
                        raise ValueError("APIè¿”å›ç©ºå“åº”")

                    embedding = response.data[0].embedding
                    if not isinstance(embedding, list) or len(embedding) == 0:
                        raise ValueError("æ— æ•ˆçš„åµŒå…¥æ ¼å¼")

                    print(f"âœ… åµŒå…¥æˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(embedding)}")
                    
                    # ç¼“å­˜ç»“æœ
                    self.cache[cache_key] = embedding
                    embeddings.append(embedding)
                    break
                except Exception as e:
                    error_msg = str(e)
                    print(f"âŒ åµŒå…¥å°è¯• {attempt+1} å¤±è´¥: {error_msg}")
                    
                    if "rate limit" in error_msg.lower():
                        print("   APIé€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…æ—¶é—´å»¶é•¿")
                        time.sleep(5)  # é€Ÿç‡é™åˆ¶æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                    
                    if attempt == 2:
                        print(f"âš ï¸ åµŒå…¥æœ€ç»ˆå¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰: {error_msg}")
                        # ä½¿ç”¨é›¶å‘é‡ä»£æ›¿ï¼Œç»´åº¦ä½¿ç”¨å¸¸è§çš„åµŒå…¥ç»´åº¦
                        if self.model_name == "text-embedding-ada-002":
                            dim = 1536  # Ada-002çš„ç»´åº¦
                        elif "text-embedding-3" in self.model_name:
                            dim = 3072 if "large" in self.model_name else 1536  # æ–°æ¨¡å‹çš„ç»´åº¦
                        else:
                            dim = 1024  # é»˜è®¤ç»´åº¦
                            
                        embeddings.append([0.0] * dim)
                    time.sleep(1)  # é‡è¯•é—´éš”
        return embeddings
    
    def get_cache_stats(self):
        """è¿”å›ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total = self.cache_hits + self.api_calls
        hit_rate = (self.cache_hits / total) * 100 if total > 0 else 0
        return {
            "cache_size": len(self.cache),
            "cache_hits": self.cache_hits,
            "api_calls": self.api_calls,
            "hit_rate_percent": hit_rate
        }
        
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        cache_size = len(self.cache)
        self.cache.clear()
        return f"å·²æ¸…é™¤ {cache_size} æ¡ç¼“å­˜åµŒå…¥"


class ReRanker(ABC):
    @abstractmethod
    def rerank(self, query: str, documents: List[str]) -> List[float]:
        pass


class CrossEncoderReRanker(ReRanker):
    def __init__(self, model_path: str):
        self.model = CrossEncoder(model_path)

    def rerank(self, query: str, documents: List[str]) -> List[float]:
        pairs = [[query, doc] for doc in documents]
        return self.model.predict(pairs).tolist()


class OnlineCrossEncoderReRanker(ReRanker):
    def __init__(self, model_name: str, api_key: Optional[str] = None, base_url: Optional[str] = None):
        self.model_name = model_name
        self.client = OpenAI(api_key=api_key, base_url=base_url)

    def rerank(self, query: str, documents: List[str]) -> List[float]:
        scores = []
        for doc in documents:
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system",
                         "content": "æ‚¨æ˜¯ä¸€ä¸ªå¸®åŠ©è¯„ä¼°æ–‡æ¡£ä¸æŸ¥è¯¢ç›¸å…³æ€§çš„åŠ©æ‰‹ã€‚è¯·ä»…è¿”å›ä¸€ä¸ª0åˆ°1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡æœ¬ã€‚"},
                        {"role": "user", "content": f"æŸ¥è¯¢ï¼š{query}\næ–‡æ¡£ï¼š{doc}\nè¯·è¯„ä¼°è¯¥æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-1ï¼‰ï¼š"}
                    ]
                )
                content = response.choices[0].message.content.strip()
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ•°å€¼
                match = re.search(r"0?\.\d+|\d\.?\d*", content)
                if match:
                    score = float(match.group())
                    score = max(0.0, min(1.0, score))  # ç¡®ä¿åˆ†æ•°åœ¨0-1ä¹‹é—´
                else:
                    score = 0.0  # è§£æå¤±è´¥é»˜è®¤å€¼
            except Exception as e:
                score = 0.0  # å¼‚å¸¸å¤„ç†
            scores.append(score)
        return scores


class RAG:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None or not kwargs.get('singleton', True):
            cls._instance = super(RAG, cls).__new__(cls)
        return cls._instance

    def __init__(self,
                 embedding_model: EmbeddingModel = None,
                 reranker: Optional[ReRanker] = None,
                 singleton: bool = True
                 ):
        if not hasattr(self, 'initialized'):
            self.embedding_model = embedding_model
            self.reranker = reranker
            self.index = None
            self.documents = []
            self.initialized = True

    def initialize_index(self, dim: int = 1024):
        """æ˜¾å¼åˆå§‹åŒ–ç´¢å¼•ï¼Œé˜²æ­¢ç©ºæŒ‡é’ˆå¼‚å¸¸"""
        if self.index is None:
            self.index = faiss.IndexFlatL2(dim)
            print(f"å·²åˆå§‹åŒ–FAISSç´¢å¼•ï¼Œç»´åº¦: {dim}")

    def add_documents(self, documents=None, texts: List[str] = None):
        """
        æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•ï¼Œç¡®ä¿ä¸é‡å¤æ·»åŠ 
        
        Args:
            documents: æ–‡æ¡£å­—å…¸ {key: value} æˆ–é”®å€¼å¯¹åˆ—è¡¨ [(key1, value1), (key2, value2), ...]
            texts: æ–‡æœ¬åˆ—è¡¨ [text1, text2, ...]
        """
        if not documents and not texts:
            print("æ²¡æœ‰æä¾›ä»»ä½•æ–‡æ¡£")
            return
        
        # è¯¦ç»†è®°å½•è¾“å…¥æƒ…å†µ
        if documents:
            print(f"æ¥æ”¶åˆ°documentså‚æ•°ï¼Œç±»å‹: {type(documents)}")
            if isinstance(documents, dict):
                print(f"å­—å…¸åŒ…å« {len(documents)} ä¸ªé”®å€¼å¯¹")
            elif isinstance(documents, list):
                print(f"åˆ—è¡¨åŒ…å« {len(documents)} ä¸ªé¡¹ç›®")
                for i, item in enumerate(documents[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                    print(f"  ç¤ºä¾‹é¡¹ç›®{i+1}: {type(item)}, {str(item)[:50]}...")
        if texts:
            print(f"æ¥æ”¶åˆ°textså‚æ•°ï¼ŒåŒ…å« {len(texts)} ä¸ªæ–‡æœ¬")
            for i, text in enumerate(texts[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"  ç¤ºä¾‹æ–‡æœ¬{i+1}: {text[:50]}...")
        
        # å‡†å¤‡è¦æ·»åŠ çš„æ–‡æœ¬
        new_texts = []
        skipped_texts = []
        
        # å¤„ç†æ–‡æ¡£å‚æ•° - æ”¯æŒå­—å…¸æˆ–é”®å€¼å¯¹åˆ—è¡¨
        if documents:
            if hasattr(documents, 'items'):  # å­—å…¸ç±»å‹
                for key, value in documents.items():
                    print(f"æ£€æŸ¥é”®å€¼å¯¹ - é”®: {key[:30]}..., å€¼: {value[:30]}...")
                    
                    # æ£€æŸ¥é”®æ˜¯å¦é‡å¤
                    if key in self.documents:
                        print(f"  è·³è¿‡é‡å¤é”®: {key[:30]}...")
                        skipped_texts.append(key)
                    else:
                        print(f"  æ·»åŠ æ–°é”®: {key[:30]}...")
                        new_texts.append(key)
                    
                    # æ£€æŸ¥å€¼æ˜¯å¦é‡å¤
                    if value in self.documents:
                        print(f"  è·³è¿‡é‡å¤å€¼: {value[:30]}...")
                        skipped_texts.append(value)
                    else:
                        print(f"  æ·»åŠ æ–°å€¼: {value[:30]}...")
                        new_texts.append(value)
                    
            elif isinstance(documents, list):  # é”®å€¼å¯¹åˆ—è¡¨æˆ–æ™®é€šåˆ—è¡¨
                for item in documents:
                    if isinstance(item, tuple) and len(item) == 2:
                        key, value = item
                        print(f"æ£€æŸ¥é”®å€¼å¯¹ - é”®: {key[:30]}..., å€¼: {value[:30]}...")
                        
                        # ä½¿ç”¨å’Œä¸Šé¢ç›¸åŒçš„æ£€æŸ¥é€»è¾‘
                        if key in self.documents:
                            print(f"  è·³è¿‡é‡å¤é”®: {key[:30]}...")
                            skipped_texts.append(key)
                        else:
                            print(f"  æ·»åŠ æ–°é”®: {key[:30]}...")
                            new_texts.append(key)
                        
                        if value in self.documents:
                            print(f"  è·³è¿‡é‡å¤å€¼: {value[:30]}...")
                            skipped_texts.append(value)
                        else:
                            print(f"  æ·»åŠ æ–°å€¼: {value[:30]}...")
                            new_texts.append(value)
                    else:
                        # å•é¡¹æ–‡æ¡£ï¼Œä¸æ˜¯é”®å€¼å¯¹
                        text = str(item)
                        print(f"æ£€æŸ¥å•é¡¹: {text[:50]}...")
                        if text in self.documents:
                            print(f"  è·³è¿‡é‡å¤é¡¹: {text[:30]}...")
                            skipped_texts.append(text)
                        else:
                            print(f"  æ·»åŠ æ–°é¡¹: {text[:30]}...")
                            new_texts.append(text)
            else:
                print(f"ä¸æ”¯æŒçš„documentsç±»å‹: {type(documents)}")
                raise ValueError(f"documentså‚æ•°å¿…é¡»æ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œæ”¶åˆ°: {type(documents)}")
        
        # å¤„ç†åˆ—è¡¨å½¢å¼çš„æ–‡æ¡£
        if texts:
            for text in texts:
                print(f"æ£€æŸ¥æ–‡æœ¬: {text[:50]}...")
                if text in self.documents:
                    print(f"  è·³è¿‡é‡å¤æ–‡æœ¬: {text[:30]}...")
                    skipped_texts.append(text)
                else:
                    print(f"  æ·»åŠ æ–°æ–‡æœ¬: {text[:30]}...")
                    new_texts.append(text)
        
        # å¦‚æœæ²¡æœ‰æ–°æ–‡æ¡£ï¼Œç›´æ¥è¿”å›
        if not new_texts:
            print(f"æ²¡æœ‰æ–°æ–‡æ¡£éœ€è¦æ·»åŠ ï¼Œå·²è·³è¿‡ {len(skipped_texts)} ä¸ªé‡å¤æ–‡æ¡£")
            return
        
        # å¯¹æ–‡æ¡£è¿›è¡Œå»é‡
        print(f"åˆæ­¥æ”¶é›†äº† {len(new_texts)} ä¸ªæ–°æ–‡æ¡£ï¼Œè¿›è¡Œå†…éƒ¨å»é‡...")
        unique_texts = []
        seen = set()
        for text in new_texts:
            normalized = re.sub(r'\s+', '', text.lower())
            normalized = re.sub(r'[^\w\s]', '', normalized)
            
            if normalized not in seen:
                seen.add(normalized)
                unique_texts.append(text)
            else:
                print(f"  å†…éƒ¨å»é‡: è·³è¿‡ {text[:30]}...")
                skipped_texts.append(text)
        
        print(f"å†…éƒ¨å»é‡åå‰©ä½™ {len(unique_texts)} ä¸ªæ–‡æ¡£")
        
        # æ£€æŸ¥ç°æœ‰æ–‡æ¡£é›†åˆ
        current_docs_normals = {re.sub(r'\s+', '', doc.lower()): i 
                             for i, doc in enumerate(self.documents)}
        current_docs_normals = {re.sub(r'[^\w\s]', '', key): val 
                             for key, val in current_docs_normals.items()}
        
        truly_new_texts = []
        for text in unique_texts:
            normalized = re.sub(r'\s+', '', text.lower())
            normalized = re.sub(r'[^\w\s]', '', normalized)
            
            if normalized in current_docs_normals:
                original_doc = self.documents[current_docs_normals[normalized]]
                print(f"  è§„èŒƒåŒ–å»é‡: è·³è¿‡ '{text[:30]}...'ï¼ŒåŒ¹é…å·²æœ‰ '{original_doc[:30]}...'")
                skipped_texts.append(text)
            else:
                truly_new_texts.append(text)
        
        # æœ€ç»ˆæ±‡æ€»
        print(f"æœ€ç»ˆå»é‡ç»Ÿè®¡:")
        print(f"  - åŸå§‹æ–‡æ¡£æ•°: {len(new_texts)} ä¸ª")
        print(f"  - è·³è¿‡é‡å¤: {len(skipped_texts)} ä¸ª")
        print(f"  - å¾…æ·»åŠ æ–°æ–‡æ¡£: {len(truly_new_texts)} ä¸ª")
        
        # å¦‚æœå»é‡åæ²¡æœ‰æ–°æ–‡æ¡£ï¼Œç›´æ¥è¿”å›
        if not truly_new_texts:
            print("å»é‡åæ²¡æœ‰æ–°æ–‡æ¡£éœ€è¦æ·»åŠ ")
            return
        
        print(f"å‡†å¤‡æ·»åŠ  {len(truly_new_texts)} ä¸ªæ–°æ–‡æ¡£åˆ°ç´¢å¼•")
        
        # æ‰“å°éƒ¨åˆ†æ–°æ–‡æ¡£ç¤ºä¾‹
        for i, text in enumerate(truly_new_texts[:3]):
            print(f"  æ–°æ–‡æ¡£ {i+1}: {text[:100]}...")
        
        # ç”ŸæˆåµŒå…¥
        print("å¼€å§‹ç”Ÿæˆæ–‡æ¡£åµŒå…¥...")
        embeddings = self.embedding_model.embed(truly_new_texts)
        if not embeddings or len(embeddings) == 0:
            print("âš ï¸ åµŒå…¥æ¨¡å‹è¿”å›ç©ºå€¼")
            raise ValueError("åµŒå…¥æ¨¡å‹è¿”å›ç©ºå€¼")

        # è½¬æ¢å¹¶æ£€æŸ¥ç»´åº¦
        embeddings = np.array(embeddings, dtype=np.float32)
        print(f"åµŒå…¥ç»´åº¦: {embeddings.shape}")
        
        # åˆå§‹åŒ–æˆ–æ£€æŸ¥ç´¢å¼•ç»´åº¦
        if self.index is None:
            dim = embeddings.shape[1]
            self.index = faiss.IndexFlatL2(dim)
            print(f"åˆå§‹åŒ–FAISSç´¢å¼•ï¼Œç»´åº¦: {dim}")
        elif embeddings.shape[1] != self.index.d:
            print(f"âš ï¸ åµŒå…¥ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.index.d}ï¼Œå®é™… {embeddings.shape[1]}")
            raise ValueError(f"åµŒå…¥ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.index.d}ï¼Œå®é™…{embeddings.shape[1]}")

        # æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•
        self.index.add(embeddings)
        self.documents.extend(truly_new_texts)
        
        print(f"ç´¢å¼•æ›´æ–°å®Œæˆï¼Œå½“å‰ç´¢å¼•åŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£")

    def query(self, query: str, top_k: int = 5, rerank: bool = False) -> List[str]:
        """æŸ¥è¯¢ç›¸å…³æ–‡æ¡£"""
        if not self.documents:
            return []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_model.embed([query])[0]
        
        # æœç´¢ç›¸ä¼¼æ–‡æ¡£
        D, I = self.index.search(np.array([query_embedding]), min(top_k, len(self.documents)))
        results = [self.documents[i] for i in I[0]]
        
        # ä½¿ç”¨é›†åˆå»é‡
        unique_results = list(set(results))
        
        # å¦‚æœéœ€è¦é‡æ’åº
        if rerank and self.reranker and len(unique_results) > 1:
            scores = self.reranker.rerank(query, unique_results)
            scored_results = list(zip(unique_results, scores))
            scored_results.sort(key=lambda x: x[1], reverse=True)
            unique_results = [r[0] for r in scored_results]
        
        print(f"RAGæŸ¥è¯¢: æ‰¾åˆ°{len(unique_results)}æ¡å»é‡ç»“æœï¼Œä»{len(results)}ä¸ªå€™é€‰ç»“æœä¸­")
        return unique_results

    def deduplicate_documents(self):
        """
        æ¸…ç†ç´¢å¼•ä¸­çš„é‡å¤æ–‡æ¡£
        è¿™å°†é‡å»ºæ•´ä¸ªç´¢å¼•ï¼Œç¡®ä¿æ¯ä¸ªæ–‡æ¡£åªå‡ºç°ä¸€æ¬¡
        """
        if not self.documents:
            print("æ²¡æœ‰æ–‡æ¡£ï¼Œæ— éœ€å»é‡")
            return
        
        print(f"å¼€å§‹å»é‡ï¼Œå½“å‰æ–‡æ¡£æ•°: {len(self.documents)}")
        
        # è·å–å½“å‰æ–‡æ¡£å¹¶åˆ›å»ºè§„èŒƒåŒ–æ˜ å°„
        original_count = len(self.documents)
        
        # æ›´å¼ºçš„è§„èŒƒåŒ–å’Œå»é‡
        unique_docs = []
        seen_normalized = set()
        
        for doc in self.documents:
            # åˆ›å»ºè§„èŒƒåŒ–ç‰ˆæœ¬ï¼ˆç§»é™¤ç©ºæ ¼ã€æ ‡ç‚¹å¹¶è½¬ä¸ºå°å†™ï¼‰
            normalized = re.sub(r'\s+', '', doc.lower())
            normalized = re.sub(r'[^\w\s]', '', normalized)
            
            if normalized not in seen_normalized:
                seen_normalized.add(normalized)
                unique_docs.append(doc)
            else:
                print(f"æ‰¾åˆ°é‡å¤æ–‡æ¡£: {doc[:50]}...")
        
        new_count = len(unique_docs)
        
        if original_count == new_count:
            print("æ²¡æœ‰å‘ç°é‡å¤æ–‡æ¡£")
            return
        
        print(f"å‘ç°{original_count - new_count}ä¸ªé‡å¤æ–‡æ¡£ï¼Œæ­£åœ¨é‡å»ºç´¢å¼•...")
        
        # é‡ç½®å½“å‰ç´¢å¼•å’Œæ–‡æ¡£
        self.documents = []
        if self.index:
            dim = self.index.d
            self.index = faiss.IndexFlatL2(dim)
        
        # é‡æ–°æ·»åŠ å»é‡åçš„æ–‡æ¡£
        for i, doc in enumerate(unique_docs):
            print(f"é‡æ–°æ·»åŠ æ–‡æ¡£ {i+1}/{len(unique_docs)}: {doc[:30]}...")
        
        # ä½¿ç”¨textså‚æ•°æ·»åŠ æ–‡æ¡£
        self.add_documents(texts=unique_docs)
        print(f"ç´¢å¼•é‡å»ºå®Œæˆï¼Œä»{original_count}ä¸ªæ–‡æ¡£å‡å°‘åˆ°{len(self.documents)}ä¸ªå”¯ä¸€æ–‡æ¡£")
